---
title: "Class 08: Mini Project"
author: "A16442048"
format: pdf
---

# 1.Exploratory Data Analysis

```{r}
fna.data <- "WisconsinCancer.csv"
```

```{r}
wisc.df <- read.csv(fna.data, row.names=1)
```

```{r}
wisc.data <- wisc.df[,-1]
```
```{r}
diagnosis <- wisc.df[,1]
diagnosis
```
>Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

>Q2. How many of the observations have a malignant diagnosis?

```{r}
mal <- "M"
sum(diagnosis == "M")
```


>Q3. How many variables/features in the data are suffixed with _mean?

```{r}
all_col_names <- colnames(wisc.df)
mean <- "mean"
```
```{r}
mean_suffix <- grep(mean,all_col_names,value ="TRUE")
mean_suffix
```
10

# 2. Principal Component Analysis

```{r}
colMeans(wisc.data)

apply(wisc.data,2,sd)
```
```{r}
wisc.pr <- prcomp(wisc.data)
```
```{r}
summary(wisc.pr)
```
>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27%

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs

```{r}
biplot(wisc.pr)
```
>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

It has 4 axes, everything is large but the plot si small so there is much overlap which makes it hard to understand.
```{r}
library(ggplot2)
```

```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,2] , col=as.factor(diagnosis),
     xlab = "PC1", ylab = "PC2")
```

>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,3] , col=as.factor(diagnosis),
     xlab = "PC1", ylab = "PC3")
```
When PC1 is plotted against PC3 rather than PC2, it leans higher on the y axis. THey are also both skewed to the right of the plot a lot. They also both separate benign form malignant diagnoses.

```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
```

```{r}
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
total_var <- sum(wisc.pr$sdev^2)
total_var
```

```{r}
pve <- (wisc.pr$sdev^2) / total_var
pve
```

```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

-4.778078e-05

>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
cumulative_pve <- cumsum(pve)
which(cumulative_pve >= 0.8)[1]
```

# 3.Hierarchical Clustering

```{r}
scaled <- scale(wisc.data)
```
```{r}
data.dist <- dist(scaled)
```
```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

>Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h = 19, col = "red", lty = 2)
```
19

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```
>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

You can try multiple numbers between 2 and 10 and then decide which one is a better match.

>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

"ward.D2" because it shows the cleanest clusters and is the easiest to interpret for me

# 5.Combining Methods

```{r}
cum.var <- cumsum(summary(wisc.pr)$importance[2, ])
num.comp <- which.max(cum.var >= 0.9)
```

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "ward.D2")
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```
```{r}
table(grps, diagnosis)
```
```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=as.factor(diagnosis))
```
```{r}
g <- as.factor(grps)
levels(g)
```
```{r}
g <- relevel(g,2)
levels(g)
```
```{r}
plot(wisc.pr$x[,1:2], col=g)
```
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "ward.D2")
```
```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

>Q15. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
compare <- table(wisc.pr.hclust.clusters, as.numeric(factor(diagnosis)))
print(compare)
```
>Q17. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses?

The `wisc.km$cluster` is better in my opinion because the `wisc.hclust.clusters` breaks it down into more groups but with less meaning, the numbers for each group are very insignificantly changed from the first.

# 6. Sensitivity/Specificity

>Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?


```{r}
#K-Means

#Sensitivity:
  37 / (37 + 175) 
#Specificity: 
  14 / (14 + 343) 
```
```{r}
#Hierarchical

# Sensitivity:
  5 / (5 + 165)
# Specificity:
12 / (12 + 343)
```

K-means for both.

# 7. Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

>Q18. Which of these new patients should we prioritize for follow up based on your results?

The ones labeled in red.

```{r}
sessionInfo()
```

